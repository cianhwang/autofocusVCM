{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import numpy as np\n",
    "#import h5py\n",
    "#import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "#from scipy import signal\n",
    "from tensorflow.python.keras.layers import Input,Conv2D,Concatenate,Flatten,Dense,LeakyReLU,Dropout, ReLU\n",
    "from tensorflow.python.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from tensorflow.python.keras.activations import relu\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "# from tensorflow.python.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "#from tensorflow.python.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
    "#from tensorflow.python.keras.models import Sequential\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 2\n",
    "d_min = 1\n",
    "d_max = 10\n",
    "blur_filter_size = 11\n",
    "blur_range = 10\n",
    "image_size = 512\n",
    "epochs = 300\n",
    "batch_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_path = 'train_data_uint8_512_10000_vcm_alpha.npy'\n",
    "# test_data_path = 'real_data_uint8_512_vcm_10000_alpha.npy'\n",
    "# train_label_path = 'train_label_uint8_512_10000_vcm_alpha.npy'\n",
    "# test_label_path = 'real_label_uint8_512_vcm_10000_alpha.npy'\n",
    "\n",
    "train_data_path = 'train_data_8000_raw.npy'\n",
    "test_data_path = 'real_data_2000_raw.npy'\n",
    "train_label_path = 'train_label_8000_raw.npy'\n",
    "test_label_path = 'real_label_2000_raw.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(train_data_path)#[:10000, :, :, :]\n",
    "test_data = np.load(test_data_path)\n",
    "\n",
    "train_label = np.load(train_label_path)#[:10000, :]\n",
    "test_label = np.load(test_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 512, 512, 1)\n",
      "(8000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "# test_data = test_data[1::2, :, :, :]\n",
    "# test_label = test_label[1::2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data -= np.mean(train_data, axis = 0)\n",
    "# train_data /= np.std(train_data, axis = 0)\n",
    "# test_data -= np.mean(test_data, axis = 0)\n",
    "# test_data /= np.std(test_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pre_normalize(im):\n",
    "#     # https://www.learnopencv.com/image-quality-assessment-brisque/\n",
    "#     blurred = cv2.GaussianBlur(im, (7, 7), 1.166) # apply gaussian blur to the image\n",
    "#     blurred_sq = blurred * blurred\n",
    "#     sigma = cv2.GaussianBlur(im * im, (7, 7), 1.166) \n",
    "#     sigma = (sigma - blurred_sq) ** 0.5\n",
    "#     sigma = sigma + 1.0/255 # to make sure the denominator doesn't give DivideByZero Exception\n",
    "#     structdis = (im - blurred)/sigma # final MSCN(i, j) image\n",
    "#     t = im - blurred\n",
    "#     t = (t-t.min())/(t.max() - t.min())\n",
    "#     return t\n",
    "\n",
    "# for iii in range(train_data.shape[0]):\n",
    "#     train_data[iii, :, :, 0] = pre_normalize(train_data[iii, :, :, 0].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# for idx in range(10):\n",
    "#     plt.imshow(train_data[idx, :, :, 0].astype(np.float32))\n",
    "#     plt.show()\n",
    "#     print(train_label[idx, :])\n",
    "# #print(train_data.dtype)\n",
    "# # batch_features = np.zeros((image_size//2, image_size//2, 4))\n",
    "# # batch_features[:, :, 0] = train_data[idx, 1::2, ::2, 0]\n",
    "# # batch_features[:, :, 1] = train_data[idx, ::2, ::2, 0]\n",
    "# # batch_features[:, :, 2] = train_data[idx, 1::2, 1::2, 0]\n",
    "# # batch_features[:, :, 3] = train_data[idx, ::2, 1::2, 0]\n",
    "# # plt.imshow(batch_features[:, :, 3]*16383)\n",
    "\n",
    "#     plt.imshow(test_data[idx, :, :, 0].astype(np.float32))\n",
    "#     plt.show()\n",
    "#     print(test_label[idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/qian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Tensor(\"out/BiasAdd:0\", shape=(?, 1), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 512, 512, 1)       0         \n",
      "_________________________________________________________________\n",
      "Conv1_1 (Conv2D)             (None, 64, 64, 4)         260       \n",
      "_________________________________________________________________\n",
      "Conv2_1 (Conv2D)             (None, 16, 16, 8)         520       \n",
      "_________________________________________________________________\n",
      "Conv3_1 (Conv2D)             (None, 4, 4, 8)           1032      \n",
      "_________________________________________________________________\n",
      "flat (Flatten)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "d1 (Dense)                   (None, 1024)              132096    \n",
      "_________________________________________________________________\n",
      "lr1 (LeakyReLU)              (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "d2 (Dense)                   (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "lr2 (LeakyReLU)              (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "d3 (Dense)                   (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "lr3 (LeakyReLU)              (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 412,821\n",
      "Trainable params: 412,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# In[7]:\n",
    "input_image1 = Input(shape=(512, 512, 1), name = \"input\")\n",
    "\n",
    "layer1_1 = Conv2D(4, (8, 8), 8,padding='valid',activation=relu, name=\"Conv1_1\")(input_image1)\n",
    "#layer1_1 = LeakyReLU(0.1)(layer1_1)\n",
    "\n",
    "layer2_1 = Conv2D(8, (4, 4), 4,padding='valid',activation=relu, name=\"Conv2_1\")(layer1_1)\n",
    "#layer2_1 = LeakyReLU(0.1)(layer2_1)\n",
    "\n",
    "layer3_1 = Conv2D(8, (4, 4), 4,padding='valid',activation=relu, name=\"Conv3_1\")(layer2_1)\n",
    "\n",
    "flattened = Flatten(name=\"flat\")(layer3_1)\n",
    "dense1 = Dense(1024, name=\"d1\")(flattened)\n",
    "ReLU1 = LeakyReLU(0.1, name=\"lr1\")(dense1)\n",
    "dp1 = Dropout(0.5)(ReLU1)\n",
    "\n",
    "dense2 = Dense(256, name=\"d2\")(dp1)\n",
    "ReLU2 = LeakyReLU(0.1, name=\"lr2\")(dense2)\n",
    "dp2 = Dropout(0.5)(ReLU2)\n",
    "\n",
    "dense3 = Dense(64, name=\"d3\")(dp2)\n",
    "ReLU3 = LeakyReLU(0.1, name=\"lr3\")(dense3)\n",
    "dp3 = Dropout(0.5)(ReLU3)\n",
    "\n",
    "output_position = Dense(1, name=\"out\")(dp3)\n",
    "print(output_position)\n",
    "\n",
    "model = Model(inputs=input_image1, outputs=output_position)\n",
    "model.summary()\n",
    "\n",
    "tcbc = TensorBoard(log_dir='1')\n",
    "\n",
    "filepath=\"raw_models/gen/n_{epoch:03d}-{val_loss:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(features, labels, batch_size):\n",
    "    while True:\n",
    "        for i in np.arange(0, features.shape[0] - batch_size, batch_size):\n",
    "            # choose random index in features\n",
    "            ################!!!\n",
    "#             batch_features = np.zeros((batch_size, image_size//2, image_size//2, 4))\n",
    "#             batch_features[:, :, :, 0] = features[i:i+batch_size, 1::2, ::2, 0]\n",
    "#             batch_features[:, :, :, 1] = features[i:i+batch_size, ::2, ::2, 0]\n",
    "#             batch_features[:, :, :, 2] = features[i:i+batch_size, 1::2, 1::2, 0]\n",
    "#             batch_features[:, :, :, 3] = features[i:i+batch_size, ::2, 1::2, 0]\n",
    "            batch_features = np.zeros((batch_size, image_size, image_size, 1))\n",
    "            batch_features = features[i:i+batch_size, :, :, :]\n",
    "            batch_features = batch_features.astype('float16')#/255###########################\n",
    "            batch_labels = abs(labels[i:i+batch_size, 1:2] - labels[i:i+batch_size, :1])*100\n",
    "            yield (batch_features, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0001)\n",
    "model.compile(loss='mse', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     train_data, train_label, test_size=0.01, random_state=2233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qian/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 4.8352\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.83523, saving model to raw_models/gen/n_001-4.835.hdf5\n",
      "285/285 [==============================] - 8s 29ms/step - loss: 4.8975 - val_loss: 4.8352\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 4.2131\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.83523 to 4.21314, saving model to raw_models/gen/n_002-4.213.hdf5\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 2.3962 - val_loss: 4.2131\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 4.6944\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.21314\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 2.0992 - val_loss: 4.6944\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 3.4596\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.21314 to 3.45961, saving model to raw_models/gen/n_004-3.460.hdf5\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.8862 - val_loss: 3.4596\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 3.6170\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.45961\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.8712 - val_loss: 3.6170\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 3.5909\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.45961\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 1.6756 - val_loss: 3.5909\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 3.3221\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.45961 to 3.32207, saving model to raw_models/gen/n_007-3.322.hdf5\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.5654 - val_loss: 3.3221\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 2.7923\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.32207 to 2.79232, saving model to raw_models/gen/n_008-2.792.hdf5\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 1.5946 - val_loss: 2.7923\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 3.3065\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.79232\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 1.6420 - val_loss: 3.3065\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 2.3146\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.79232 to 2.31459, saving model to raw_models/gen/n_010-2.315.hdf5\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 1.5130 - val_loss: 2.3146\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 2.4749\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.31459\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 1.4409 - val_loss: 2.4749\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 2.1160\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.31459 to 2.11599, saving model to raw_models/gen/n_012-2.116.hdf5\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 1.4231 - val_loss: 2.1160\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 2.3699\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.11599\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 1.3691 - val_loss: 2.3699\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 2.8951\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.11599\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 1.3461 - val_loss: 2.8951\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 2.1447\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.11599\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.2960 - val_loss: 2.1447\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.8969\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.11599 to 1.89686, saving model to raw_models/gen/n_016-1.897.hdf5\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 1.2977 - val_loss: 1.8969\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.8098\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.89686 to 1.80979, saving model to raw_models/gen/n_017-1.810.hdf5\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.2931 - val_loss: 1.8098\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 2.4490\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.80979\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.2483 - val_loss: 2.4490\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 2.2563\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.80979\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.2315 - val_loss: 2.2563\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 2.7253\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.80979\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.2457 - val_loss: 2.7253\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.7758\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.80979 to 1.77582, saving model to raw_models/gen/n_021-1.776.hdf5\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.1917 - val_loss: 1.7758\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 2.1629\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.77582\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 1.1913 - val_loss: 2.1629\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.9539\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.77582\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.1249 - val_loss: 1.9539\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.8858\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.77582\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.1583 - val_loss: 1.8858\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 2.0010\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.77582\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.1154 - val_loss: 2.0010\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.8683\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.77582\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.1079 - val_loss: 1.8683\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.7582\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.77582 to 1.75817, saving model to raw_models/gen/n_027-1.758.hdf5\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.0664 - val_loss: 1.7582\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 2.1780\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.75817\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.1002 - val_loss: 2.1780\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 2.0288\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.75817\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.0456 - val_loss: 2.0288\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.5816\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.75817 to 1.58164, saving model to raw_models/gen/n_030-1.582.hdf5\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.0639 - val_loss: 1.5816\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.6370\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.58164\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.0281 - val_loss: 1.6370\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.8007\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.58164\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 1.0076 - val_loss: 1.8007\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4730\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.58164 to 1.47305, saving model to raw_models/gen/n_033-1.473.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 7s 26ms/step - loss: 1.0173 - val_loss: 1.4730\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.7124\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.47305\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 1.0353 - val_loss: 1.7124\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.7679\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.47305\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.9967 - val_loss: 1.7679\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3237\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.47305 to 1.32374, saving model to raw_models/gen/n_036-1.324.hdf5\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.9760 - val_loss: 1.3237\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.5180\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.32374\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.9633 - val_loss: 1.5180\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.5068\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.32374\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.9657 - val_loss: 1.5068\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2238\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.32374 to 1.22381, saving model to raw_models/gen/n_039-1.224.hdf5\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.9449 - val_loss: 1.2238\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.7735\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.9466 - val_loss: 1.7735\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2615\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.9186 - val_loss: 1.2615\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.6020\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.9339 - val_loss: 1.6020\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4306\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.9201 - val_loss: 1.4306\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2747\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8961 - val_loss: 1.2747\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.7158\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8937 - val_loss: 1.7158\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3198\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8678 - val_loss: 1.3198\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2837\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8752 - val_loss: 1.2837\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.4820\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8559 - val_loss: 1.4820\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4534\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8555 - val_loss: 1.4534\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.5348\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8476 - val_loss: 1.5348\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.7504\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.8593 - val_loss: 1.7504\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4279\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8443 - val_loss: 1.4279\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.9185\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8268 - val_loss: 1.9185\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.7785\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.22381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8405 - val_loss: 1.7785\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0238\n",
      "\n",
      "Epoch 00055: val_loss improved from 1.22381 to 1.02381, saving model to raw_models/gen/n_055-1.024.hdf5\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.8270 - val_loss: 1.0238\n",
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.6458\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7920 - val_loss: 1.6458\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.6690\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7922 - val_loss: 1.6690\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3926\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.8055 - val_loss: 1.3926\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4029\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7774 - val_loss: 1.4029\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4294\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.7799 - val_loss: 1.4294\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4041\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.8047 - val_loss: 1.4041\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.8167\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7641 - val_loss: 1.8167\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.9667\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7884 - val_loss: 1.9667\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.7490\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.7739 - val_loss: 1.7490\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.5974\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.7636 - val_loss: 1.5974\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4112\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.7390 - val_loss: 1.4112\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.8458\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.7629 - val_loss: 1.8458\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4016\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.7582 - val_loss: 1.4016\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.5194\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7590 - val_loss: 1.5194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3706\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7280 - val_loss: 1.3706\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4750\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.7164 - val_loss: 1.4750\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.7277\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7307 - val_loss: 1.7277\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.7120\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7326 - val_loss: 1.7120\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.5358\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7279 - val_loss: 1.5358\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2127\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7215 - val_loss: 1.2127\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.6213\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7239 - val_loss: 1.6213\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.3790\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7037 - val_loss: 1.3790\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.7423\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7231 - val_loss: 1.7423\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.5818\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7101 - val_loss: 1.5818\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.6273\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7105 - val_loss: 1.6273\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3627\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6745 - val_loss: 1.3627\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4911\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6691 - val_loss: 1.4911\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4714\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6985 - val_loss: 1.4714\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3096\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.7108 - val_loss: 1.3096\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.5778\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6819 - val_loss: 1.5778\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4139\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6738 - val_loss: 1.4139\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4517\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6709 - val_loss: 1.4517\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2413\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6527 - val_loss: 1.2413\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4384\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.02381\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6742 - val_loss: 1.4384\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0192\n",
      "\n",
      "Epoch 00090: val_loss improved from 1.02381 to 1.01921, saving model to raw_models/gen/n_090-1.019.hdf5\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6393 - val_loss: 1.0192\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1898\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6301 - val_loss: 1.1898\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1966\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6659 - val_loss: 1.1966\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.7845\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6463 - val_loss: 1.7845\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1023\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.6317 - val_loss: 1.1023\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.5759\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6313 - val_loss: 1.5759\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3445\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6298 - val_loss: 1.3445\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1370\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.6409 - val_loss: 1.1370\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4391\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6350 - val_loss: 1.4391\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2212\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.6401 - val_loss: 1.2212\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4863\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6264 - val_loss: 1.4863\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.3627\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6357 - val_loss: 1.3627\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2213\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.6063 - val_loss: 1.2213\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2641\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.6287 - val_loss: 1.2641\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1834\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5887 - val_loss: 1.1834\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3149\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.6164 - val_loss: 1.3149\n",
      "Epoch 106/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0619\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6171 - val_loss: 1.0619\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1704\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5887 - val_loss: 1.1704\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.0911\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6163 - val_loss: 1.0911\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1009\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5896 - val_loss: 1.1009\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0715\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.6100 - val_loss: 1.0715\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0946\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6032 - val_loss: 1.0946\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1275\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5996 - val_loss: 1.1275\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1993\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6105 - val_loss: 1.1993\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0786\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6124 - val_loss: 1.0786\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.0435\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5912 - val_loss: 1.0435\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0278\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5783 - val_loss: 1.0278\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0420\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5583 - val_loss: 1.0420\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1275\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5730 - val_loss: 1.1275\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0323\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.6082 - val_loss: 1.0323\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1579\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5940 - val_loss: 1.1579\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1758\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5690 - val_loss: 1.1758\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2174\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5624 - val_loss: 1.2174\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1315\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5591 - val_loss: 1.1315\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2435\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5640 - val_loss: 1.2435\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0919\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5561 - val_loss: 1.0919\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2034\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.01921\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5453 - val_loss: 1.2034\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.9793\n",
      "\n",
      "Epoch 00127: val_loss improved from 1.01921 to 0.97933, saving model to raw_models/gen/n_127-0.979.hdf5\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5729 - val_loss: 0.9793\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.9500\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.97933 to 0.95002, saving model to raw_models/gen/n_128-0.950.hdf5\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.5560 - val_loss: 0.9500\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1480\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5622 - val_loss: 1.1480\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0998\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5543 - val_loss: 1.0998\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2143\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.5518 - val_loss: 1.2143\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.0170\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.5581 - val_loss: 1.0170\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2497\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5425 - val_loss: 1.2497\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3189\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5243 - val_loss: 1.3189\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2558\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5470 - val_loss: 1.2558\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1730\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5268 - val_loss: 1.1730\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2517\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5211 - val_loss: 1.2517\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1916\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5464 - val_loss: 1.1916\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.9716\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 26ms/step - loss: 0.5362 - val_loss: 0.9716\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2168\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.95002\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5290 - val_loss: 1.2168\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.9349\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.95002 to 0.93493, saving model to raw_models/gen/n_141-0.935.hdf5\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5144 - val_loss: 0.9349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.9454\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.93493\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5175 - val_loss: 0.9454\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0848\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.93493\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5255 - val_loss: 1.0848\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2158\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.93493\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5169 - val_loss: 1.2158\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1939\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.93493\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5405 - val_loss: 1.1939\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1598\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.93493\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5454 - val_loss: 1.1598\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1110\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.93493\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5051 - val_loss: 1.1110\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.9949\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.93493\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5044 - val_loss: 0.9949\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3729\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.93493\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5048 - val_loss: 1.3729\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1249\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.93493\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5082 - val_loss: 1.1249\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1415\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.93493\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.5011 - val_loss: 1.1415\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.9022\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.93493 to 0.90222, saving model to raw_models/gen/n_152-0.902.hdf5\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4878 - val_loss: 0.9022\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1651\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5148 - val_loss: 1.1651\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1308\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5106 - val_loss: 1.1308\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.0057\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4925 - val_loss: 1.0057\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0079\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4910 - val_loss: 1.0079\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0374\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5027 - val_loss: 1.0374\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1595\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4946 - val_loss: 1.1595\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0232\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4844 - val_loss: 1.0232\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3632\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4789 - val_loss: 1.3632\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0576\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.5041 - val_loss: 1.0576\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1285\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4731 - val_loss: 1.1285\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1440\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4790 - val_loss: 1.1440\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1191\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4684 - val_loss: 1.1191\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0913\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4870 - val_loss: 1.0913\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2214\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4621 - val_loss: 1.2214\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2372\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4813 - val_loss: 1.2372\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1042\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4773 - val_loss: 1.1042\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.3592\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4802 - val_loss: 1.3592\n",
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.9567\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4804 - val_loss: 0.9567\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2130\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4852 - val_loss: 1.2130\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0008\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4799 - val_loss: 1.0008\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2972\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4743 - val_loss: 1.2972\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0324\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4670 - val_loss: 1.0324\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.4957\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4706 - val_loss: 1.4957\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1764\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4620 - val_loss: 1.1764\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.0974\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4691 - val_loss: 1.0974\n",
      "Epoch 178/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2592\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4708 - val_loss: 1.2592\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1429\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4656 - val_loss: 1.1429\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2193\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4614 - val_loss: 1.2193\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.9726\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4525 - val_loss: 0.9726\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.3029\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4599 - val_loss: 1.3029\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1777\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4681 - val_loss: 1.1777\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1798\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4487 - val_loss: 1.1798\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0968\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4703 - val_loss: 1.0968\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1896\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4499 - val_loss: 1.1896\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2828\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4597 - val_loss: 1.2828\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3005\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4615 - val_loss: 1.3005\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.3153\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4485 - val_loss: 1.3153\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0426\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4654 - val_loss: 1.0426\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1004\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4536 - val_loss: 1.1004\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0034\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4409 - val_loss: 1.0034\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2203\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4640 - val_loss: 1.2203\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1287\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4528 - val_loss: 1.1287\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2586\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4328 - val_loss: 1.2586\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0986\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4464 - val_loss: 1.0986\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2825\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4446 - val_loss: 1.2825\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1328\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4393 - val_loss: 1.1328\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0178\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4376 - val_loss: 1.0178\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.3728\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4371 - val_loss: 1.3728\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.9500\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4510 - val_loss: 0.9500\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.3192\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4292 - val_loss: 1.3192\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.0944\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4379 - val_loss: 1.0944\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2216\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4401 - val_loss: 1.2216\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.2121\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4272 - val_loss: 1.2121\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2607\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4495 - val_loss: 1.2607\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2053\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4144 - val_loss: 1.2053\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2455\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4189 - val_loss: 1.2455\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3147\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4210 - val_loss: 1.3147\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0760\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4373 - val_loss: 1.0760\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4049\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3971 - val_loss: 1.4049\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1782\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4179 - val_loss: 1.1782\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.5781\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4110 - val_loss: 1.5781\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1760\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4315 - val_loss: 1.1760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2766\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4238 - val_loss: 1.2766\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1371\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4144 - val_loss: 1.1371\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2272\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4157 - val_loss: 1.2272\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1195\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4229 - val_loss: 1.1195\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0751\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4197 - val_loss: 1.0751\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3611\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4288 - val_loss: 1.3611\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.9743\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4173 - val_loss: 0.9743\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4878\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4242 - val_loss: 1.4878\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0504\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4131 - val_loss: 1.0504\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3489\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4063 - val_loss: 1.3489\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1605\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4196 - val_loss: 1.1605\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4740\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4185 - val_loss: 1.4740\n",
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1489\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4041 - val_loss: 1.1489\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2972\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4225 - val_loss: 1.2972\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.3560\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4306 - val_loss: 1.3560\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.1741\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4055 - val_loss: 1.1741\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4425\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4050 - val_loss: 1.4425\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0314\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4058 - val_loss: 1.0314\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2586\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4197 - val_loss: 1.2586\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3716\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3990 - val_loss: 1.3716\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.4196\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4201 - val_loss: 1.4196\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0906\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4070 - val_loss: 1.0906\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3896\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4073 - val_loss: 1.3896\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.3341\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4129 - val_loss: 1.3341\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1201\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4218 - val_loss: 1.1201\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3073\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4085 - val_loss: 1.3073\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.0549\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4038 - val_loss: 1.0549\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3734\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4123 - val_loss: 1.3734\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0275\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4136 - val_loss: 1.0275\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2119\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4027 - val_loss: 1.2119\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1695\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4020 - val_loss: 1.1695\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.2589\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3963 - val_loss: 1.2589\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2443\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3901 - val_loss: 1.2443\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0577\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3932 - val_loss: 1.0577\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1530\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3950 - val_loss: 1.1530\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0808\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4007 - val_loss: 1.0808\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2763\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3911 - val_loss: 1.2763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.9634\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3995 - val_loss: 0.9634\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2576\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3973 - val_loss: 1.2576\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2356\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3966 - val_loss: 1.2356\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3136\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4014 - val_loss: 1.3136\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0624\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.4123 - val_loss: 1.0624\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1851\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3855 - val_loss: 1.1851\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2056\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4025 - val_loss: 1.2056\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1208\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4010 - val_loss: 1.1208\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4066\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3978 - val_loss: 1.4066\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1485\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4035 - val_loss: 1.1485\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2288\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3922 - val_loss: 1.2288\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1455\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3899 - val_loss: 1.1455\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2551\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.4027 - val_loss: 1.2551\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0935\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3821 - val_loss: 1.0935\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2201\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3740 - val_loss: 1.2201\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1191\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3813 - val_loss: 1.1191\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2179\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3774 - val_loss: 1.2179\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1001\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3757 - val_loss: 1.1001\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.0903\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3902 - val_loss: 1.0903\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2089\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3886 - val_loss: 1.2089\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0485\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3984 - val_loss: 1.0485\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2135\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3978 - val_loss: 1.2135\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1067\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3761 - val_loss: 1.1067\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2688\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3764 - val_loss: 1.2688\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2073\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3809 - val_loss: 1.2073\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2976\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3789 - val_loss: 1.2976\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1250\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3687 - val_loss: 1.1250\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1613\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3715 - val_loss: 1.1613\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2485\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3809 - val_loss: 1.2485\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0113\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3733 - val_loss: 1.0113\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2040\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3834 - val_loss: 1.2040\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0539\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3740 - val_loss: 1.0539\n",
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2307\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3649 - val_loss: 1.2307\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0689\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3621 - val_loss: 1.0689\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2658\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3722 - val_loss: 1.2658\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.0885\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3812 - val_loss: 1.0885\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.1310\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3739 - val_loss: 1.1310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1261\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3661 - val_loss: 1.1261\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1708\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3725 - val_loss: 1.1708\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2066\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3668 - val_loss: 1.2066\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0872\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3725 - val_loss: 1.0872\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2369\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3681 - val_loss: 1.2369\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.9750\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3587 - val_loss: 0.9750\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2370\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3616 - val_loss: 1.2370\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1110\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3683 - val_loss: 1.1110\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2449\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3735 - val_loss: 1.2449\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0153\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3753 - val_loss: 1.0153\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.1616\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 24ms/step - loss: 0.3665 - val_loss: 1.1616\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1525\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.90222\n",
      "285/285 [==============================] - 7s 25ms/step - loss: 0.3600 - val_loss: 1.1525\n"
     ]
    }
   ],
   "source": [
    "# model.fit(train_data[:,:,:,0:1].astype('float16')/255.0, abs(train_label[:,1:2]-train_label[:,0:1])/100, \n",
    "#           epochs = 100,batch_size = batch_size, #validation_split=0.2,\n",
    "#           validation_data=(test_data[:,:,:,0:1].astype('float16')/255.0, abs(test_label[:,1:2]-test_label[:,0:1])/100), \n",
    "#           verbose=1, callbacks = [tcbc, checkpoint])\n",
    "history = model.fit_generator(data_gen(train_data, train_label, batch_size), \n",
    "                    steps_per_epoch = train_data.shape[0]//batch_size, epochs = epochs,\n",
    "                    validation_data=data_gen(test_data, test_label, batch_size), \n",
    "                    validation_steps = batch_size,verbose=1, callbacks = [tcbc, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('auto_hist_raw', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "# with open(r\"raw_models/trainHistoryDict_raw\", \"rb\") as input_file:\n",
    "#     history = pickle.load(input_file)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt.plot(history['acc'])\n",
    "# plt.plot(history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "#plt.scatter([88], history['val_loss'][88], marker='*', c = 'r')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model_512.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_label[:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "model = load_model('raw_models/gen/n_152-0.902.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# idxx = 89\n",
    "# plt.imshow(test_data[idxx, :, :, 0])\n",
    "# print(test_label[idxx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(test_data[idxx:idxx+1, :, :, :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# batch_features_ = np.ones(y_test.shape)\n",
    "# for i in range(2000//64-1): #\n",
    "#     batch_features = X_test[64*i:64*i+64,:,:,:]\n",
    "# #    batch_features = np.zeros((64, image_size//2, image_size//2, 4))\n",
    "# #     batch_features[:, :, :, 0] = X_test[64*i:64*i+64, 1::2, ::2, 0]\n",
    "# #     batch_features[:, :, :, 1] = X_test[64*i:64*i+64, ::2, ::2, 0]\n",
    "# #     batch_features[:, :, :, 2] = X_test[64*i:64*i+64, 1::2, 1::2, 0]\n",
    "# #     batch_features[:, :, :, 3] = X_test[64*i:64*i+64, ::2, 1::2, 0]\n",
    "#     batch_features_[64*i:64*i+64, :] = model.predict(batch_features)\n",
    "\n",
    "# plt.scatter(abs(y_test[:1936, 0] - y_test[:1936, 1])*100, batch_features_[:1936, 0] - abs(y_test[:1936, 0] - y_test[:1936, 1])*100, color='black', s = 0.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6000//64-1): #\n",
    "#     train_pred[64*i:64*i+64, :, :, :] = model.predict(X_train[64*i:64*i+64, :, :, :1])\n",
    "#     plt.scatter(abs(y_train[64*i:64*i+64, 0] - y_train[64*i:64*i+64, 1])*100, train_pred[:, 0] - abs(y_train[64*i:64*i+64, 0] - y_train[64*i:64*i+64, 1])*100, color='black', s = 0.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred[test_pred<0]= 0\n",
    "# print(test_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.imshow(test_data[0, :, :, 0])\n",
    "# print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(abs(test_label[:, 0] - test_label[:, 1]), test_pred[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
